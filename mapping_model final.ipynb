{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as ds\n",
    "from model import RNN,LSTM,GRU\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=pd.read_csv('ESGOverallData Trainset.csv')\n",
    "trainset.dropna(inplace=True)\n",
    "testset = pd.read_csv('ESGOverallData Testset.csv')\n",
    "testset.dropna(inplace=True)\n",
    "predict = pd.read_csv('ESGOverallData Predictset.csv')\n",
    "predict.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyTicker</th>\n",
       "      <th>Year</th>\n",
       "      <th>CSV.Length</th>\n",
       "      <th>ESGData</th>\n",
       "      <th>BloombergOverall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ON</td>\n",
       "      <td>2016</td>\n",
       "      <td>1131</td>\n",
       "      <td>[[0.0055219400674104, 0.0078070629388093, 0.00...</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FFIV</td>\n",
       "      <td>2020</td>\n",
       "      <td>844</td>\n",
       "      <td>[[0.004199112765491, 0.0238876864314079, 0.021...</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>2016</td>\n",
       "      <td>451</td>\n",
       "      <td>[[0.0117304930463433, 0.015321378596127, 0.010...</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>2016</td>\n",
       "      <td>692</td>\n",
       "      <td>[[0.0129457851871848, 0.0162092838436365, 0.01...</td>\n",
       "      <td>4.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYNA</td>\n",
       "      <td>2018</td>\n",
       "      <td>697</td>\n",
       "      <td>[[0.0032364279031753, 0.0065713287331163, 0.00...</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>KLAC</td>\n",
       "      <td>2016</td>\n",
       "      <td>1156</td>\n",
       "      <td>[[0.0019834351260215, 0.0017482317052781, 0.00...</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>AMD</td>\n",
       "      <td>2017</td>\n",
       "      <td>1360</td>\n",
       "      <td>[[0.0024161378387361, 0.001941418624483, 0.003...</td>\n",
       "      <td>6.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>AGS</td>\n",
       "      <td>2018</td>\n",
       "      <td>259</td>\n",
       "      <td>[[0.0055219400674104, 0.0078070629388093, 0.00...</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>SYNA</td>\n",
       "      <td>2015</td>\n",
       "      <td>380</td>\n",
       "      <td>[[0.0129457851871848, 0.0162092838436365, 0.01...</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>UI</td>\n",
       "      <td>2017</td>\n",
       "      <td>1174</td>\n",
       "      <td>[[0.0087282545864582, 0.0131738856434822, 0.00...</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CompanyTicker  Year  CSV.Length  \\\n",
       "0              ON  2016        1131   \n",
       "1            FFIV  2020         844   \n",
       "2            NVDA  2016         451   \n",
       "3            CSCO  2016         692   \n",
       "4            SYNA  2018         697   \n",
       "..            ...   ...         ...   \n",
       "266          KLAC  2016        1156   \n",
       "267           AMD  2017        1360   \n",
       "268           AGS  2018         259   \n",
       "269          SYNA  2015         380   \n",
       "270            UI  2017        1174   \n",
       "\n",
       "                                               ESGData  BloombergOverall  \n",
       "0    [[0.0055219400674104, 0.0078070629388093, 0.00...              4.16  \n",
       "1    [[0.004199112765491, 0.0238876864314079, 0.021...              2.70  \n",
       "2    [[0.0117304930463433, 0.015321378596127, 0.010...              5.68  \n",
       "3    [[0.0129457851871848, 0.0162092838436365, 0.01...              4.53  \n",
       "4    [[0.0032364279031753, 0.0065713287331163, 0.00...              1.65  \n",
       "..                                                 ...               ...  \n",
       "266  [[0.0019834351260215, 0.0017482317052781, 0.00...              2.33  \n",
       "267  [[0.0024161378387361, 0.001941418624483, 0.003...              6.09  \n",
       "268  [[0.0055219400674104, 0.0078070629388093, 0.00...              1.23  \n",
       "269  [[0.0129457851871848, 0.0162092838436365, 0.01...              1.84  \n",
       "270  [[0.0087282545864582, 0.0131738856434822, 0.00...              1.60  \n",
       "\n",
       "[271 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.3], [0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.1]],   # report1 representation\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.3]],\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.3], [0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.5]],   # report2 representation\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.3], [0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.5]]    # report2 representation\n",
    "#]\n",
    "\n",
    "def trim_data(data, max_length=3):\n",
    "    trimmed_data = []\n",
    "    for sample in data:\n",
    "        if len(sample) > max_length:\n",
    "            # 找出最里面第四个维度的值\n",
    "            fourth_values = [item[3] for item in sample]\n",
    "            # 找出要保留的前max_length个最大值的索引\n",
    "            indices_to_keep = sorted(range(len(fourth_values)), key=lambda i: fourth_values[i], reverse=False)[:max_length]\n",
    "            # 按原顺序保留这些索引对应的元素\n",
    "            trimmed_report = [sample[i] for i in sorted(indices_to_keep)]\n",
    "            trimmed_data.append(trimmed_report)\n",
    "        else:\n",
    "            trimmed_data.append(sample)\n",
    "    return trimmed_data\n",
    "\n",
    "#trimmed_data = trim_data(data)\n",
    "#print(trimmed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression dataset\n",
    "class MyDataset1(ds.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.samples = X\n",
    "        self.labels = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        label = [self.labels[index]]\n",
    "        #print(label)\n",
    "        sample=torch.Tensor(sample).float()\n",
    "        label= torch.Tensor(label).float()\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(df):\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    for index,row in df.iterrows():\n",
    "        x_train.append(ast.literal_eval(row[\"ESGData\"]))\n",
    "        y_train.append(float(row[\"BloombergOverall\"]))\n",
    "    return x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_time_series(data, target_length=None, padding_value=0.0):\n",
    "    # Determine the target length if not provided\n",
    "    final_result=[]\n",
    "    if target_length is None:\n",
    "        target_length = max(len(sample) for sample in data)\n",
    "\n",
    "    for sample in data:\n",
    "        if len(sample) < target_length:\n",
    "            for _ in range(target_length - len(sample)):\n",
    "                sample.append([padding_value for _ in range(4)])\n",
    "            final_result.append(sample)\n",
    "        \n",
    "        elif len(sample) > target_length:\n",
    "            fourth_values = [item[3] for item in sample]\n",
    "            indices_to_keep = sorted(range(len(fourth_values)), key=lambda i: fourth_values[i], reverse=False)[:target_length]\n",
    "            trimmed_report = [sample[i] for i in sorted(indices_to_keep)]\n",
    "            final_result.append(trimmed_report)\n",
    "        \n",
    "        else:\n",
    "            final_result.append(sample)\n",
    "\n",
    "    return final_result\n",
    "            \n",
    "\n",
    "# Example usage:\n",
    "#data = [\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.3], [0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.1]],   # report1 representation\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.3]],\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.3],[0.1, 0.2, 0.3, 0.3]],\n",
    "#   [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.3], [0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.5]],   # report2 representation\n",
    "#    [[0.1, 0.2, 0.3, 0.5], [0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.3], [0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.5]]    # report2 representation\n",
    "#]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "def r_squared(y_true, y_pred,y_mean):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - y_mean) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "def information_coefficient(y_true, y_pred):\n",
    "    # 计算IC值\n",
    "    numerator = torch.corrcoef(y_true, y_pred)[0, 1]  # 相关系数\n",
    "    denominator = torch.std(y_true) * torch.std(y_pred)  # 标准差的乘积\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "def loop(model,train_loader,test_loader,optimizer,criterion,device,E=20):\n",
    "    loss_all=[] # List to store the loss for each batch in the epoch\n",
    "    print(f'Device is using:{device}')\n",
    "    for epoch in range(E): \n",
    "        loss_epoch=[]\n",
    "        #print(epoch)\n",
    "        for samples, labels in train_loader:\n",
    "            samples=samples.to(device)\n",
    "            labels=labels.to(device)\n",
    "            #print(samples.shape)\n",
    "            #print(labels.shape)\n",
    "            model.train()               # Set the flag to training mode\n",
    "            optimizer.zero_grad()       # Clear the gradients of all optimized tensors\n",
    "            outputs = model(samples)    # Forward pass\n",
    "            loss = criterion(outputs.squeeze(), labels) # Compute the loss\n",
    "            loss.backward()     # Backward pass\n",
    "            optimizer.step()    # Update the parameters\n",
    "            loss_epoch.append(float(loss.cpu()))    # Store the loss value for the batch\n",
    "            #print(outputs)\n",
    "        #print(f'MSE{sum(loss_epoch)/len(loss_epoch)}') #训练集平均loss\n",
    "        loss_all.append(sum(loss_epoch)/len(loss_epoch))    # Store the average loss for the epoch\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    result=[]   # List to store the predictions on the test dataset\n",
    "    TrueValue = []\n",
    "    mae_loss = 0\n",
    "    r2_score = 0\n",
    "    mape_loss = 0\n",
    "    mse_loss = 0  # Initialize MSE loss\n",
    "    ic = 0\n",
    "\n",
    "    total_labels = torch.tensor([]).to(device) # Initialize total_labels with an empty tensor\n",
    "    for samples, true in test_loader:\n",
    "        samples=samples.to(device)  # Move samples to the specified device\n",
    "        outputs = model(samples) # 预测值  Forward pass to get predictions\n",
    "        predictions = float(outputs.cpu()) # loat(outputs.cpu())\n",
    "        result.append(predictions)\n",
    "        TrueValue.append(float(true.cpu()))\n",
    "\n",
    "        # Update total_labels with the new batch of labels\n",
    "        total_labels = torch.cat((total_labels, labels))\n",
    "        # Calculate the current y_mean\n",
    "        y_mean = torch.mean(total_labels)\n",
    "\n",
    "        # Compute MAE\n",
    "        mae_loss += mean_absolute_error(true, predictions)        \n",
    "        # Compute R^2\n",
    "        r2_score += r_squared(true, predictions,y_mean)        \n",
    "        # Compute MAPE\n",
    "        mape_loss += mean_absolute_percentage_error(true, predictions)        \n",
    "        # Compute MSE\n",
    "        mse_loss += mean_squared_error(true, predictions)\n",
    "\n",
    "    # Average the metrics over the test dataset\n",
    "    num_samples = len(test_loader)\n",
    "    mae_loss /= num_samples\n",
    "    r2_score /= num_samples\n",
    "    mape_loss /= num_samples\n",
    "    mse_loss /= num_samples\n",
    "    ic, _ = pearsonr(result, TrueValue)\n",
    "    rank_ic, _ = spearmanr(result, TrueValue)\n",
    "\n",
    "    AverageMetrics = [mae_loss,r2_score,mape_loss,mse_loss,ic]\n",
    "        \n",
    "    return result,loss_all,model,AverageMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.MSELoss(), the criterion variable can be used with other loss functions depending on the type of problem you are trying to solve. \n",
    "\n",
    "nn.CrossEntropyLoss() for classification tasks, which computes the cross-entropy loss between the predicted and true class labels.\n",
    "\n",
    "nn.BCEWithLogitsLoss() for binary classification tasks, which combines the sigmoid function with the binary cross-entropy loss.\n",
    "\n",
    "nn.NLLLoss() for classification tasks, which computes the negative log likelihood loss.\n",
    "\n",
    "nn.HingeLoss() for classification tasks, especially for support vector machines, which computes the hinge loss between the predicted and true values.\n",
    "\n",
    "nn.L1Loss() for regression tasks, which computes the mean absolute error loss.\n",
    "nn.SmoothL1Loss() for regression tasks, which computes a smooth version of the mean absolute error loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch_size=180 #128\n",
    "learning_rate=0.001      #0.000001\n",
    "epoch=150\n",
    "hidden_num=50\n",
    "output_dim=1 #if None put ''\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2bf0b00a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y=generate_data(trainset)\n",
    "x = pad_time_series(x, 800, padding_value=0.25)\n",
    "train_dataset = MyDataset1(x,y)\n",
    "x,y=generate_data(testset)\n",
    "x = pad_time_series(x, 800, padding_value=0.25)\n",
    "test_dataset = MyDataset1(x,y)\n",
    "\n",
    "train_loader=ds.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=ds.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
    "criterion = nn.MSELoss()\n",
    "display(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qjy/anaconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using:cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qjy/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([180, 1])) that is different to the input size (torch.Size([180])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/qjy/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([91, 1])) that is different to the input size (torch.Size([91])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN predict [2.872462272644043, 2.872462272644043, 2.872462272644043, 2.8720884323120117, 2.870527744293213, 2.872462272644043, 2.872096538543701, 2.8724617958068848, 2.871939182281494, 2.8720126152038574, 2.872462272644043, 2.872462272644043, 2.8720977306365967, 2.872462272644043, 2.8717355728149414, 2.8726511001586914, 2.872462272644043, 2.8721137046813965, 2.872462272644043, 2.872462272644043, 2.872044086456299, 2.872121572494507, 2.872061014175415, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.87211012840271, 2.872248411178589, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.8721182346343994, 2.8724122047424316, 2.872462272644043, 2.872220516204834, 2.872462272644043, 2.8719472885131836, 2.8721132278442383, 2.8720736503601074, 2.872462272644043, 2.872073173522949, 2.872462272644043, 2.8724639415740967, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.8720712661743164, 2.872436761856079, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.872138738632202, 2.872462272644043, 2.872462272644043, 2.8721091747283936, 2.8721203804016113, 2.87258243560791, 2.8720412254333496, 2.8712565898895264, 2.872462272644043, 2.8721365928649902, 2.872462272644043, 2.872462272644043, 2.872093915939331, 2.872462272644043, 2.872371196746826, 2.8720765113830566, 2.872098445892334, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.8725385665893555, 2.8720226287841797, 2.872462272644043, 2.872101068496704, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.872462272644043, 2.8720977306365967, 2.8725669384002686, 2.872462272644043, 2.872462272644043, 2.8720974922180176, 2.872462272644043]\n",
      "loss RNN [9.430265426635742, 8.31938123703003, 6.950361013412476, 6.16779351234436, 5.104453802108765, 3.8585197925567627, 3.1463035345077515, 2.6022485494613647, 2.1224557161331177, 2.108974874019623, 2.0655738711357117, 2.1137203574180603, 2.088236093521118, 2.1081926822662354, 2.133511185646057, 2.1840686202049255, 2.1529911756515503, 2.127953052520752, 2.0473430156707764, 2.0936102867126465, 2.0420135259628296, 2.092555344104767, 1.9151338338851929, 1.9928481578826904, 2.0423935651779175, 2.0303540229797363, 2.087324321269989, 2.086645722389221, 2.0410799980163574, 1.9853582382202148, 2.0951091647148132, 1.9914363622665405, 1.9687414765357971, 1.9095810651779175, 1.9583154916763306, 2.0178168416023254, 2.0228858590126038, 2.107416272163391, 2.0124707221984863, 1.9515957236289978, 1.9814253449440002, 1.93927800655365, 1.9668133854866028, 1.9865352511405945, 2.0062209963798523, 2.015615224838257, 2.04315584897995, 2.0460349917411804, 1.953421711921692, 1.957269549369812, 2.047205090522766, 2.0276677012443542, 1.9972463846206665, 2.04506778717041, 2.0324987173080444, 1.9679561257362366, 2.044099271297455, 1.9563668966293335, 2.017362594604492, 1.9652372598648071, 1.9406552910804749, 1.9606842398643494, 2.0073976516723633, 2.033167004585266, 2.0771981477737427, 2.0832547545433044, 1.9725379347801208, 1.9728911519050598, 1.9841418862342834, 1.9849766492843628, 2.0291923880577087, 1.9516980051994324, 2.032013714313507, 1.975310742855072, 2.036964535713196, 2.033167541027069, 1.9556515216827393, 1.9944227933883667, 2.008832335472107, 1.9740334153175354, 2.0317060947418213, 2.037097990512848, 1.9513910412788391, 1.980164885520935, 2.0271183848381042, 2.064186155796051, 2.0826783776283264, 1.9968045949935913, 1.9659844040870667, 1.9600883722305298, 2.0074974298477173, 1.9828993082046509, 2.043556571006775, 1.991666853427887, 2.038718104362488, 1.903651773929596, 2.0711161494255066, 2.062898814678192, 2.077308714389801, 1.9905579090118408, 2.023897171020508, 2.0886592864990234, 2.00644189119339, 2.0243033170700073, 1.9678304195404053, 1.9951205849647522, 2.1645877361297607, 2.0631489157676697, 2.1699677109718323, 1.9537506699562073, 1.8763883113861084, 2.063857316970825, 1.9717828035354614, 1.9706387519836426, 2.0378613471984863, 2.0464894771575928, 1.993122398853302, 1.9585933685302734, 1.9943665862083435, 2.002293884754181, 1.9415169954299927, 2.0704206824302673, 1.9443948864936829, 1.9945549368858337, 2.0093599557876587, 2.0530040860176086, 2.1007585525512695, 2.010304093360901, 1.950044870376587, 2.0338159799575806, 1.9457767605781555, 1.9865198731422424, 2.077892243862152, 2.1192901134490967, 1.9978067278862, 2.035563886165619, 1.9740829467773438, 1.9981757402420044, 1.975792407989502, 1.9901815056800842, 1.9837650060653687, 1.9797788858413696, 2.0429311990737915, 2.0131760835647583, 2.0428247451782227, 2.0860933661460876, 2.011964201927185, 2.0324466824531555, 1.9335763454437256, 2.1412748098373413]\n",
      "AverageMetrics RNN [tensor(1.4516), tensor(-41.5994), tensor(39.5769), tensor(2.9663), -0.11369282899102189]\n"
     ]
    }
   ],
   "source": [
    "features_dim=np.array(x).shape[2]\n",
    "modelRNN=RNN.RNN_Model(features_dim,hidden_num,output_dim).to(device)\n",
    "optimizerRNN= optim.Adam(modelRNN.parameters(), lr=learning_rate)\n",
    "resultRNN,lossRNN,modelRNN,AverageMetrics=loop(modelRNN,train_loader,test_loader,optimizerRNN,criterion,device,E=epoch)\n",
    "print(f'RNN predict {resultRNN}')\n",
    "print(f'loss RNN {lossRNN}')\n",
    "print(f'AverageMetrics RNN {AverageMetrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch_size=180 #128\n",
    "learning_rate=0.01      #0.000001\n",
    "epoch=100\n",
    "hidden_num=100\n",
    "output_dim=1 #if None put ''\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=generate_data(trainset)\n",
    "x = pad_time_series(x, 800, padding_value=0.25)\n",
    "train_dataset = MyDataset1(x,y)\n",
    "x,y=generate_data(testset)\n",
    "x = pad_time_series(x, 800, padding_value=0.25)\n",
    "test_dataset = MyDataset1(x,y)\n",
    "\n",
    "train_loader=ds.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=ds.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using:cpu\n",
      "LSTM predict [2.8284823894500732, 2.8288726806640625, 2.8285467624664307, 2.8284945487976074, 2.8288726806640625, 2.8288726806640625, 2.828462600708008, 2.8284857273101807, 2.8285152912139893, 2.8288726806640625, 2.828463554382324, 2.8288726806640625, 2.8284683227539062, 2.828460216522217, 2.8288726806640625, 2.8288726806640625, 2.828460931777954, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.82846736907959, 2.8284878730773926, 2.8284640312194824, 2.8288726806640625, 2.8284904956817627, 2.8288726806640625, 2.8288726806640625, 2.828474283218384, 2.8288726806640625, 2.8288726806640625, 2.828486442565918, 2.8288726806640625, 2.8285040855407715, 2.8288726806640625, 2.8284571170806885, 2.8288726806640625, 2.828458309173584, 2.8288726806640625, 2.8288726806640625, 2.8284575939178467, 2.8284757137298584, 2.8288726806640625, 2.828451156616211, 2.828451156616211, 2.828461170196533, 2.8288726806640625, 2.8284640312194824, 2.8288726806640625, 2.828462600708008, 2.8284668922424316, 2.828477382659912, 2.8285317420959473, 2.8284599781036377, 2.8284692764282227, 2.8288726806640625, 2.8284575939178467, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8288726806640625, 2.8285107612609863, 2.8288726806640625, 2.8288726806640625, 2.828460931777954, 2.828460216522217, 2.8288726806640625, 2.82846736907959, 2.8285269737243652, 2.8288726806640625, 2.8288726806640625, 2.8285117149353027, 2.8284621238708496, 2.8288726806640625, 2.8288726806640625, 2.828536033630371, 2.8288726806640625, 2.8288726806640625, 2.8284618854522705, 2.8288726806640625]\n",
      "loss LSTM [8.851159811019897, 3.0254805088043213, 2.5175729990005493, 2.399466395378113, 2.334347367286682, 2.1058499813079834, 2.0962000489234924, 2.083901286125183, 2.1256978511810303, 2.0666096210479736, 2.056900680065155, 2.003122627735138, 1.975093126296997, 2.02351450920105, 1.9582029581069946, 2.0480926036834717, 1.9327307343482971, 1.9606247544288635, 2.061164677143097, 2.0212284326553345, 1.929232656955719, 1.936261773109436, 2.0079418420791626, 2.0116418600082397, 1.9902759790420532, 2.0272695422172546, 1.9956396222114563, 1.984928846359253, 1.9582538604736328, 1.9886834025382996, 1.9524250626564026, 1.9218228459358215, 2.0928837060928345, 1.9456350207328796, 1.9578372836112976, 1.9742074608802795, 2.023358941078186, 1.9103229641914368, 2.0290831327438354, 2.046357274055481, 2.042406141757965, 2.090397894382477, 2.060560703277588, 2.0328664779663086, 1.991490364074707, 1.990481197834015, 1.9860508441925049, 1.973630666732788, 2.0346359610557556, 1.9789178967475891, 1.9580724835395813, 2.0454737544059753, 2.0470643043518066, 1.973857581615448, 1.9396148920059204, 2.0815683007240295, 1.934644639492035, 1.9611852765083313, 1.973323404788971, 2.0143778324127197, 2.089479923248291, 1.9849209785461426, 2.045203924179077, 2.0615806579589844, 2.0241568088531494, 1.985711932182312, 1.9799309372901917, 1.943779468536377, 2.0912962555885315, 2.022847831249237, 1.991104781627655, 2.050680458545685, 1.9507588744163513, 2.008052170276642, 1.9647109508514404, 2.0459946393966675, 2.0500305891036987, 2.0069868564605713, 2.0933948159217834, 1.9721189737319946, 1.9778726696968079, 1.9614174365997314, 2.071841835975647, 2.089909613132477, 2.0119398832321167, 2.0365559458732605, 2.0885369777679443, 2.001651406288147, 1.992499589920044, 2.0747499465942383, 1.9449858665466309, 1.9703730940818787, 1.9607248306274414, 2.0001134872436523, 2.02624648809433, 1.9865176677703857, 2.0885514616966248, 2.0145031213760376, 2.005784034729004, 2.053212583065033]\n",
      "AverageMetrics LSTM [tensor(1.4676), tensor(-1.7271), tensor(39.5336), tensor(3.0485), 0.0914913397821483]\n"
     ]
    }
   ],
   "source": [
    "features_dim=np.array(x).shape[2]\n",
    "modelLSTM=LSTM.LSTM_Model(features_dim,hidden_num,output_dim).to(device)\n",
    "optimizerLSTM= optim.Adam(modelLSTM.parameters(), lr=learning_rate)\n",
    "resultLSTM,lossLSTM,modelLSTM,AverageMetrics=loop(modelLSTM,train_loader,test_loader,optimizerLSTM,criterion,device,E=epoch)\n",
    "print(f'LSTM predict {resultLSTM}')\n",
    "print(f'loss LSTM {lossLSTM}')\n",
    "print(f'AverageMetrics LSTM {AverageMetrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch_size=150 #128\n",
    "learning_rate=0.01      #0.000001\n",
    "epoch=100\n",
    "hidden_num=100\n",
    "output_dim=1 #if None put ''\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=generate_data(trainset)\n",
    "x = pad_time_series(x, 800, padding_value=0.25)\n",
    "train_dataset = MyDataset1(x,y)\n",
    "x,y=generate_data(testset)\n",
    "x = pad_time_series(x, 800, padding_value=0.25)\n",
    "test_dataset = MyDataset1(x,y)\n",
    "\n",
    "train_loader=ds.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader=ds.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is using:cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qjy/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([150, 1])) that is different to the input size (torch.Size([150])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/qjy/anaconda3/envs/NLP/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([121, 1])) that is different to the input size (torch.Size([121])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU predict [0.08248762786388397, 0.08248761296272278, 0.05618912726640701, 0.0517645962536335, 0.08248762786388397, 0.08242166042327881, 0.05140719562768936, 0.08248763531446457, 0.08248762786388397, 0.08248762786388397, 0.08248762786388397, 0.08248763531446457, 0.053810618817806244, 0.05644446983933449, 0.08248763531446457, 0.054227281361818314, 0.054361071437597275, 0.08248763531446457, 0.05727456510066986, 0.08248763531446457, 0.05582272261381149, 0.053490884602069855, 0.08248762786388397, 0.08248763531446457, 0.05223724991083145, 0.08248763531446457, 0.08248762786388397, 0.08248794078826904, 0.05604394152760506, 0.05985166132450104, 0.08248762786388397, 0.05428798869252205, 0.054909247905015945, 0.08248762041330338, 0.056560732424259186, 0.055657707154750824, 0.05400438606739044, 0.05661492794752121, 0.08248762786388397, 0.08248762041330338, 0.05774550512433052, 0.05051557719707489, 0.08248763531446457, 0.08248762786388397, 0.05572814121842384, 0.08248762786388397, 0.08248763531446457, 0.052023228257894516, 0.08248762041330338, 0.08248762041330338, 0.05125883221626282, 0.05278708040714264, 0.05566607415676117, 0.05052139237523079, 0.08248762786388397, 0.08248763531446457, 0.08248762041330338, 0.05411135405302048, 0.08248763531446457, 0.05557026341557503, 0.08248762786388397, 0.057246144860982895, 0.05392960086464882, 0.05462309345602989, 0.08248762786388397, 0.08248762786388397, 0.08248762786388397, 0.08248782157897949, 0.06152892857789993, 0.05448906496167183, 0.0525103434920311, 0.0599508062005043, 0.08248762786388397, 0.08248762786388397, 0.05476353317499161, 0.08248762786388397, 0.08248763531446457, 0.05928824841976166, 0.08248762786388397, 0.07979349046945572, 0.08248762786388397, 0.08248762786388397, 0.08248762786388397, 0.05462232604622841, 0.055917393416166306, 0.05414332449436188, 0.08248762786388397]\n",
      "loss GRU [9.81680965423584, 9.794655799865723, 9.798889636993408, 9.824385643005371, 9.74743938446045, 9.76176643371582, 9.754592895507812, 9.761757373809814, 9.798861980438232, 9.727342128753662, 9.810344696044922, 9.831002235412598, 9.947853565216064, 9.784111499786377, 9.771953105926514, 9.77225637435913, 9.8695650100708, 9.802276134490967, 9.894099235534668, 9.920568943023682, 9.85412073135376, 9.758567810058594, 9.777418613433838, 9.81236219406128, 9.737538814544678, 9.854398250579834, 9.89776086807251, 9.795090675354004, 9.79042387008667, 9.860798835754395, 9.697441577911377, 9.741914749145508, 9.845514297485352, 9.783081531524658, 9.852382183074951, 9.738813877105713, 9.752413272857666, 9.812740325927734, 9.840924739837646, 9.80424976348877, 9.77204704284668, 9.864946365356445, 9.858800888061523, 9.865120887756348, 9.7575364112854, 9.797677040100098, 9.763208866119385, 9.804360389709473, 9.79480266571045, 9.781813621520996, 9.859854698181152, 9.85364294052124, 9.811443328857422, 9.750751495361328, 9.768307209014893, 9.7840256690979, 9.837241649627686, 9.800783634185791, 9.764271259307861, 9.844540119171143, 9.842166423797607, 9.843763828277588, 9.801763534545898, 9.666082382202148, 9.883111476898193, 9.866294384002686, 9.8577561378479, 9.8439040184021, 9.89381217956543, 9.824337482452393, 9.733380794525146, 9.84650468826294, 9.884828090667725, 9.861128807067871, 9.741852283477783, 9.787254333496094, 9.817856788635254, 9.85808515548706, 9.776792526245117, 9.7891206741333, 9.824217319488525, 9.84351634979248, 9.823443412780762, 9.822899341583252, 9.830443859100342, 9.810641288757324, 9.76339340209961, 9.845267295837402, 9.884685516357422, 9.737308502197266, 9.862789630889893, 9.864531993865967, 9.918174743652344, 9.857987403869629, 9.858452320098877, 9.832776546478271, 9.79084300994873, 9.808575630187988, 9.832874774932861, 9.844525337219238]\n",
      "AverageMetrics GRU [tensor(3.7256), tensor(-690.7651), tensor(97.7867), tensor(15.9907), 0.10033540264669523]\n"
     ]
    }
   ],
   "source": [
    "features_dim=np.array(x).shape[2]\n",
    "modelGRU=GRU.GRU_Model(features_dim,hidden_num,output_dim).to(device)\n",
    "optimizerGRU= optim.Adam(modelGRU.parameters(), lr=learning_rate)\n",
    "resultGRU,lossGRU,modelGRU,AverageMetrics=loop(modelGRU,train_loader,test_loader,optimizerGRU,criterion,device,E=epoch)\n",
    "print(f'GRU predict {resultGRU}')\n",
    "print(f'loss GRU {lossGRU}')\n",
    "print(f'AverageMetrics GRU {AverageMetrics}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
